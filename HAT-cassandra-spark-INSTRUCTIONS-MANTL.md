This demo instructions should be applicable to any MANTL cluster.

### Description
*Train classification model and recognize activity type by acceleration data*

Current implementation tends to extend [original application](http://www.duchess-france.org/analyze-accelerometer-data-with-apache-spark-and-mllib/).
Main difference from origin would be to provide additional prediction step so it would be possible to use different cassandra sources for training and prediction. Application works in two steps.
First it's required to train data recognition model. Available algorithm options are [DecisionTrees](http://spark.apache.org/docs/latest/mllib-decision-tree.html) and [RandomForests](http://spark.apache.org/docs/latest/mllib-ensembles.html#random-forests). Cassandra table is being used as a source storage.
After successful execution of this step model will be saved to HDFS. Also possibility af classification error will be saved into train-summary.txt file in base directory for each of currently supported algorithms respectively.
On the next step it's possible to start classification process.
Data which need to be recognised can be sourced from separate Cassandra table as a batch defined by timestamp range. Make sure:

 * range belongs to a single activity
 * range consists of some defined range of records. For data, generated by [hat-data-generator](https://github.com/CiscoCloud/mantl-apps/tree/master/useful-apps/spark-useful-apps/hat/hat-data-generator) range of min 15 values should be used, while for the real data set - at least 120 records.
 
In case not adhering this requirements result cannot be guaranteed to be adequate.
Classification result will be displayed into stdout log as a numeric double:
  Walking => 0
  Jogging => 1
  Standing => 2
  Sitting => 3
  Upstairs => 4
  Downstairs => 5
Correctness can be checked by comparing result and activity value from test table.  

### Requirements

Infrastructure:

* Spark
* HDFS
* Cassandra

Data:

Training data as Cassandra table compatible with the one, produced by [hat-data-generator](https://github.com/CiscoCloud/mantl-apps/tree/master/useful-apps/spark-useful-apps/hat/hat-data-generator).


### Tested on:

* Java 1.7.0_67
* Maven 3.3.3 (project assembly)
* Spark 1.3.0 (job runtime)
* Cassandra 2.2.3
* Datasets generated by [hat-data-generator](https://github.com/CiscoCloud/mantl-apps/tree/master/useful-apps/spark-useful-apps/hat/hat-data-generator) with amount ~10^6 records.

## Prerequisites

* Make sure your user has home directory on HDFS (or any other directory with write permissions).

The below steps are based on the assumptions:

1) user's home directory on HDFS is `/user/$USER` where `$USER` is the current user logged on to the gateway host.

2) Output Cassandra table has already been created with `CREATE TABLE users (user_id int,activity text,timestamp bigint,acc_x double,acc_y double,acc_z double, PRIMARY KEY ((user_id,activity),timestamp));` schema.

3) Table data generated by [hat-data-generator](https://github.com/CiscoCloud/mantl-apps/blob/master/useful-apps/spark-useful-apps/hat/hat-data-generator)

4) Base directory: `hdfs:///user/$USER/data/hat-output`

 * Login to a gateway host and run the below commands:
 
```
$ git clone https://github.com/CiscoCloud/mantl-apps.git
$ cd mantl-apps/useful-apps/spark-useful-apps/hat
$ mvn clean install -Pprod
ls -al hat-cassandra-spark/target/hat-cassandra-spark-1.0.jar
```
 
 * Copy jar to HDFS:
 
```
hdfs dfs -mkdir -p mantl-apps/useful-apps
hdfs dfs -put -f hat-cassandra-spark/target/hat-cassandra-spark-1.0.jar mantl-apps/useful-apps
hdfs dfs -ls mantl-apps/useful-apps/hat-cassandra-spark-1.0.jar
```

### Configure
Application can be configured from command line providing parameters

* **--host** - required, specify it to point out Cassandra host, example:
```
--host localhost:9042
```
* **--table** - required, specify it to point out Cassadnra table full name including keyspace, example:
```
--table keyspace.table
```
* **--basedir** - required, base dir on HDFS application data to be stored or read from, example:
```
--basedir hdfs://localhost/user/examples/files-out"
```
* **--users** - (Training step only) required, number of users to operate on, example:
```
--users 20
```
* **--timerange** - (Prediction step only) required, timerange of activity to be classified, IMPORTANT: must belong to single activity and not overlap with others, otherwise correct result not guaranteed, example:
```
--timerange 26328835250667:26329781790102
```
* **--algo** - (Prediction step only) required, defines algorithm that will be used for classification, possible values: DecisionTree, RandomForest
```
--algo DecisionTree
```
* --master - optional, specify it to point out spark master URI, by default will be governed by --master option of spark-submit command which would be required in case not providing it application, example: 
```
--master spark://localhost:7077
```
* --name - optional, Application display name, default: Cassandra-to-HDFS , example:
```
--name "Cassandra-to-HDFS"
```
* *--help* - can be used to view usage options from command line.

## Train model step

Train user model on the number of users, e.g. record batches and save it to HDFS:

```
/opt/spark/bin/spark-submit --master spark://mi-worker-004:7077 --deploy-mode cluster --class com.cisco.mantl.hat.HATTrainDriver hdfs:///user/$USER/mantl-apps/useful-apps/hat-cassandra-spark-1.0.jar --host cassandra-mantl-node.service.consul:9042 --table mykeyspace.users --basedir hdfs:///user/$USER/data/hat-output --users 3
```

Check output directory and summary file: 

```
hadoop fs -ls -h hdfs:///user/$USER/data/hat-output
hadoop fs -cat hdfs:///user/$USER/data/hat-output/train-summary.txt
```

## Predict step

Query Cassandra to get a timestamp range:

```
SELECT * FROM <table> WHERE user_id = <id> AND activity = '<activity>' LIMIT 20;
```

Copy first and last timestamps and use them in **--timerange** 

```
/opt/spark/bin/spark-submit --master spark://mi-worker-004:7077 --deploy-mode cluster --class com.cisco.mantl.hat.HATPredictDriver hdfs:///user/$USER/mantl-apps/useful-apps/hat-cassandra-spark-1.0.jar --host cassandra-mantl-node.service.consul:9042 --table mykeyspace.users --basedir hdfs:///user/$USER/data/hat-output --timerange <start timestamp>:<end timestamp> --algo <algorithm>
```

Check output directory and summary file: 

```
hadoop fs -ls -h hdfs:///user/$USER/data/hat-output
hadoop fs -cat hdfs:///user/$USER/data/hat-output/predict-summary.txt
```

File content should be:

```
Classification type : <activity>
```

Activity should be the same as used in above Select query.